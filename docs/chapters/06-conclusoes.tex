\chapter{Conclusões}
\label{chapter:conclusion}

Neste trabalho, apresentamos três diferentes algoritmos que compartilharam do mesmo objetivo: reproduzir o processamento aplicado a uma gravação sonora, em posse desta e de sua versão distorcida desejada. A dificuldade do problema emergiu principalmente de dois fatores: em primeiro lugar, as distorções eram (ou melhor, poderiam ser) não-lineares; além disso, havia a quase certeza de sinais espúrios estarem presentes na mixagem final, o que dificulta o processo de identificação. Deste modo, tornou-se imprescindível que os métodos fossem genéricos --- assim, capazes de imitar uma variedade de mapeamentos não-lineares --- e robustos a ruído.

O primeiro modelo estudado foi o Filtro de Wiener FIR causal. Como sua estratégia consiste apenas na convolução entre o sinal de entrada e os coeficientes ótimos, o filtro foi --- como esperado --- incapaz de reproduzir distorções não-lineares. Ainda assim, aplicá-lo aos sinais de teste nos permitiu ter ``valores de controle'' que pudéssemos comparar com resultados subsequentes. Além disso, um atributo realmente notável observado foi a capacidade do método de atenuar o ruído das mixagens, apresentando o melhor desempenho dentre os três filtros neste quesito. Sim, os sinais indesejados não foram completamente removidos, mas consideravelmente reduzidos a ponto de se tornarem curtos zumbidos metálicos. Isso muito provavelmente ocorreu porque o Filtro de Wiener usa todo o sinal (no nosso caso, todas as amostras de uma iteração) para estimar o vetor de correlação cruzada, o que facilitou na caracterização do ruído. Deve-se mencionar também a importância da técnica \textit{overlap-add}, a qual viabilizou a aplicação do algoritmo em sinais de áudio.

Em seguida, exploramos o Filtro de Correntropia. O filtro utiliza a função homônima para calcular estatísticas mais complexas (leia-se, não-lineares) acerca dos sinais de interesse. Em nossos testes preliminares, o filtro apresentou resultados tão bons quanto, ou até mesmo melhores que, o Filtro de Wiener, tanto para um sinal determinístico trivial quanto para um modelo autorregressivo. Porém, ao aplicarmos o método proposto, o qual também usou a técnica \textit{overlap-add}, os resultados foram de medianos para péssimos. A principal responsável por esse desempenho foi a lógica de variar o desvio $\sigma$ do núcleo gaussiano de modo a otimizar a SDR entre o bloco estimado e o bloco desejado da atual iteração. Como usamos a observação como sinal desejado, o filtro optou por estimações que preservassem ao máximo os sinais espúrios, e, assim, essas ficaram extremamente ruidosas. De todo modo, não podemos culpar totalmente o variador de $\sigma$, já que experimentos com desvio constante durante todo o processo (não apresentados no texto) resultaram em estimações piores; uma ideia não descartada, porém não desenvolvida, seria modificar o algoritmo de escolha de $\sigma$. Outro possível motivo para os resultados insatisfatórios é o fato de o cálculo da correntropia de um sinal pressupõe que ele seja estacionário no sentido estrito, o que não podemos afirmar ser verdade para gravações sonoras, mesmo para curtos intervalos de tempo. Pelo menos o algoritmo apresentou comportamento consistente: o filtro não foi capaz de reproduzir o processamento de codificação com perdas, introduzindo um ruído (similar a ruído branco) em ambos os experimentos executados, por exemplo.

O último método discutido foi o Filtro de Kalman \textit{Unscented}, implementado em sua versão dupla (ou seja, realizando a predição dos estados e dos coeficientes alternadamente). Antes de apresentarmos as conclusões, deve-se salientar a importância da escolha das funções $\mathbf{f}$ e $\mathbf{h}$ no desempenho do modelo. Em nosso caso, o polinômio de Volterra exibiu extraordinária competência na reprodução de distorções para os casos sem ruído. Porém, com o diálogo presente, as estimações foram apenas medianas. Considerando que i) o ruído $w_n$ é pressuposto branco gaussiano com variância $\sigma_{w}^2$, o que não é necessariamente verdade para a faixa de diálogo, e ii) as melhores estimações resultaram de valores descomunais para a variância --- na ordem das dezenas de milhares ---, o que fez com que a etapa de correção priorizasse as amostras previstas, não as observadas, é possível fazer sentido destes resultados: as notas baixas refletem a incapacidade do filtro de identificar as distorções a partir das observações, e o diálogo ficou abafado pois ele simplesmente foi atenuado com o resto das amostras da observação.

Mesmo que alguns bons resultados tenham sido obtidos, o problema está longe de ser resolvido, e, como trabalhos futuros, temos diferentes caminhos. Primeiramente, não se pode desconsiderar por completo o Filtro de Correntropia: além do mau desempenho ter ocorrido parcialmente por culpa do método proposto, existem variações do algoritmo que podem ser exploradas, como a \textit{Weighted Partial Regression}~\cite{pokharel-2007}, que, na literatura consultada, apresentou resultados mais robustos do que a versão convencional.\footnote{Não abordamos a \textit{Weighted Partial Regression} no trabalho pois o estudo sobre o Filtro de Correntropia já estava consideravelmente extenso.} Além disso, embora o Filtro de Kalman \textit{Unscented} tenha conseguido mimetizar bem as estimações nos casos sem diálogo, o desempenho do algoritmo para observações ruidosas foi insatisfatório; o método que melhor removeu os ruídos foi o Filtro de Wiener, que não foi capaz de identificar os mapeamentos mais complexos. Portanto, um primeiro ponto de interesse é procurar um método não-linear mais robusto a ruído (principalmente quando a SNR é baixa), ou melhorar o Filtro de Kalman \textit{Unscented}. Esta opção nos leva ao segundo ponto: o polinômio de Volterra usado para a modelagem do sistema é computacionalmente custoso, e, no nosso caso em específico, não considerou amostras de saídas passadas (em outras palavras, não era um modelo realimentado). Assim, um outro possível caminho é a remodelagem do sistema com uma função de custo computacional menor e que utilize de estimações passadas para calcular a atual (já que, idealmente, a saída estará menos ruidosa, facilitando então o processo de identificação). Por fim, os métodos propostos foram testados com distorções controladas e gravações artificiais; a execução de experimentos com exemplos reais, que muito provavelmente terão outros tipos de mapeamentos, é muito importante para que possamos saber a real capacidade de cada método.