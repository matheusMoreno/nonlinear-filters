\chapter{Filtro de Kalman \textit{Unscented}}
\label{chapter:unscented}

Finalizada nossa discussão sobre o Filtro de Correntropia, seguiremos agora para o
último método a ser explorado no projeto. O Filtro de Kalman \textit{Unscented},
apresentado originalmente em~\cite{julier-1997}, surgiu como uma alternativa ao Filtro
de Kalman Estendido~\cite{sorenson-1985}; a proposta de ambos é generalizar o Filtro de
Kalman~\cite{hayes-1996} para sistemas não-lineares. O modelo se baseia na
transformação \textit{unscented}, a qual gera um conjunto estratégico de pontos
associados à variável aleatória para estimar efetivamente a média e covariância de seu
mapeamento não-linear. Ao contrário dos outros algoritmos do trabalho, a filtragem é em
tempo real, ou seja, apenas amostras passadas (e presentes) são usadas no processo de
estimação, embora possamos utilizar informações globais dos sinais nas condições
iniciais.

A seção a seguir formula o problema, que é descrito na forma de uma representação em
espaço de estados. Então, a intuição (com as equações) por trás do Filtro de Kalman
será brevemente discutida. Em seguida, apresentaremos a transformação
\textit{unscented}, para que o algoritmo do Filtro de Kalman \textit{Unscented} possa
ser especificado. Finalmente, o modelo específico estruturado para o projeto será
definido, e os resultados dos experimentos executados serão comentados.

\section{Formulação do problema}
\label{section:unscented:formulation}

Todos os estimadores discutidos até agora objetivaram computar, a partir de um sinal de
entrada e uma observação, apenas a saída do sistema de interesse. Os filtros que
discutiremos agora se diferenciam dos outros métodos por estimarem também os
\emph{estados} do modelo. Precisamos então formular o novo problema em questão.

Na literatura, é possível encontrar diferentes formulações associadas ao algoritmo;
muitas vezes, não há sinal de entrada no sistema, ou os ruídos são considerados
não-aditivos. A discussão do capítulo será quase que inteiramente baseada no modelo
apresentado no artigo original do Filtro de Kalman
\textit{Unscented}~\cite{julier-1997}, já que suas hipóteses são suficientes para o
contexto do trabalho. Considere então o sistema não-linear discreto definido por
\begin{align}
	\label{eq:unscented:system-dynamic}
	\mathbf{x}_{n+1} & = \mathbf{f}(\mathbf{x}_n, \mathbf{u}_n, \mathbf{v}_n),  \\
	\mathbf{z}_n     & = \mathbf{h}(\mathbf{x}_n, \mathbf{u}_n) + \mathbf{w}_n,
	\symbl{$\mathbf{x}_n$}{Vetor de estados do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{u}_n$}{Vetor de entrada do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{v}_n$}{Vetor de ruído de processo do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{z}_n$}{Vetor de observação do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{w}_n$}{Vetor de ruído de observação do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{f}$}{Função que descreve a dinâmica dos estados do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{h}$}{Função que descreve a dinâmica da saída do sistema modelado pelo Filtro de Kalman \textit{Unscented}}
\end{align}
onde $\mathbf{f}$ e $\mathbf{h}$ são funções vetoriais não-lineares (que descrevem, respectivamente, as dinâmicas dos estados e da saída), $\mathbf{x}_n \in \mathbb{R}^k$ é o vetor de estados, $\mathbf{u}_n$ é o vetor de entrada, $\mathbf{v}_n$ é o vetor de ruído de processo (resultado de perturbações e erros de modelagem), $\mathbf{z}_n$ é o vetor de observação, e $\mathbf{w}_n$ é o vetor de ruído de observação. Um sistema especificado desta forma é dito estar em sua \emph{representação em espaço de estados.} O subscrito identifica a amostra em questão do vetor ou matriz; essa convenção foi optada a fim de evitar (mais) sobrecarga de notação.

Além disso, sendo $\delta_{ij}$ um Delta de Kronecker definido de modo que $\delta_{ij}
	= 1$ se e apenas se $i = j$, outra importante pressuposição do problema é de que os
vetores de ruído $\mathbf{v}_n$ e $\mathbf{w}_n$ ambos têm média zero, são
independentes entre si, e, sendo $\mathbf{Q}_n$ e $\mathbf{R}_n$ as matrizes de
covariância de $\mathbf{v}_n$ e $\mathbf{w}_n$ respectivamente,
\begin{equation}
	E\{ \mathbf{v}_n \mathbf{v}_m^T \} = \delta_{nm} \mathbf{Q}_n,\ E\{ \mathbf{w}_n \mathbf{w}_m^T \} = \delta_{nm} \mathbf{R}_n,\ \forall\ n, m.
	\symbl{$\mathbf{Q}_n$}{Matriz de covariância do ruído de processo do Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{R}_n$}{Matriz de covariância do ruído de saída do Filtro de Kalman \textit{Unscented}}
\end{equation}

Em suma, a memória do sistema na $n$-ésima amostra --- totalmente caracterizada por
$\mathbf{x}_n$ --- é gerada a partir de uma combinação não-linear (a função
$\mathbf{f}$) entre a entrada, os estados e o ruído de processo em $n-1$. Essa memória
é combinada também com $\mathbf{u}_n$ por meio do mapeamento não-linear $\mathbf{h}$
para resultar na saída do modelo, que é corrompida por ruído aditivo e independente. Um
exemplo trivial de representação em espaço de estados pode ser feita com o Filtro de
Wiener: as $L - 1$ amostras anteriores da entrada compõem os $L - 1$ estados do modelo,
e uma combinação linear destes com a amostra atual gera a saída do estimador.

Nosso objetivo se torna então estimar os estados e a saída do sistema. Ademais, no
contexto do trabalho, temos um obstáculo a mais: \emph{não conhecemos as funções
	$\mathbf{f}$ e $\mathbf{h}$ que descrevem o processo.} Por agora, vamos pressupor que
os mapeamentos são conhecidos; retornaremos a esse ponto na
Seção~\ref{section:unscented:model}.

\section{Filtro de Kalman}

Um dos algoritmos mais utilizados para solucionar o problema formulado é o Filtro de
Kalman. Assim como no Filtro de Wiener, o método é considerado ótimo para o erro
quadrático médio entre os estados e suas estimações; porém, diferentemente daquele, o
cálculo é feito em tempo real, apenas com amostras presentes e passadas. A dedução das
equações do filtro é extensa, e pode ser encontrada em diversos livros de processamento
de sinais~\cite{diniz-2020, hayes-1996}. Por isso, essa seção será dedicada mais à
intuição do que à matemática envolvida no método.

Primeiramente, deve-se mencionar que não conseguimos medir diretamente os estados; a
única informação do sistema é a observação $\mathbf{z}_n$, e será com ela que
realizaremos as estimações. Seja então
$\hat{\mathbf{x}}_{n|n}$\symbl{$\hat{\mathbf{x}}_{n|n}$}{Estimação \textit{a
		posteriori} do vetor de estados no Filtro de Kalman \textit{Unscented}} a estimação da
$n$-ésima amostra do vetor de estados, a qual foi computada a partir de todas as
observações $\mathbf{z}_i$ até $n$ (ou seja, $0 \leq i \leq n$); assim, $n|n$ denota
que \emph{a estimação da amostra $n$ foi computada usando as últimas $n$ observações}.
Se o sistema realmente é descrito pela Equação~\eqref{eq:unscented:system-dynamic},
podemos utilizá-la para prever o próximo estado do sistema,
$\hat{\mathbf{x}}_{n+1|n}$\symbl{$\hat{\mathbf{x}}_{n+1|n}$}{Estimação \textit{a
		priori} do vetor de estados no Filtro de Kalman \textit{Unscented}}, sem nenhuma
informação futura; esta é a etapa de \emph{predição} do algoritmo. Porém, há um detalhe
pertinente: todos os sinais presentes no sistema são interpretados como sequências
aleatórias (principalmente por conta dos ruídos), e, idealmente, a predição deve
satisfazer a seguinte condição:
\begin{equation}
	\hat{\mathbf{x}}_{n+1|n} = E\{ \mathbf{f}(\mathbf{x}_n, \mathbf{u}_n, \mathbf{v}_n) | \mathbf{z}_n, \dots, \mathbf{z}_0 \}.
	\label{eq:unscented:kalman-first}
\end{equation}
Em outras palavras, a predição deve ser a média de todos os possíveis valores para o próximo estado, dadas as observações coletadas até então. Logo, embora seja possível aplicar a Equação~\eqref{eq:unscented:system-dynamic} diretamente em $\hat{\mathbf{x}}_{n|n}$, essa abordagem não é estatisticamente adequada. Da mesma forma, a próxima amostra da matriz de covariância do vetor de estados $\mathbf{P}^{\mathbf{x}\mathbf{x}}_{n+1|n}$ também pode ser antecipada, considerando a mesma condição de otimalidade:
\begin{equation}
	\mathbf{P}^{\mathbf{x}\mathbf{x}}_{n+1|n} = E\{ (\mathbf{x}_{n+1} - \hat{\mathbf{x}}_{n+1|n})(\mathbf{x}_{n+1} - \hat{\mathbf{x}}_{n+1|n})^T | \mathbf{z}_n, \dots, \mathbf{z}_0 \}.
	\symbl{$\mathbf{P}^{\cdot\cdot}_{n+1|n}$}{Estimação \textit{a priori} da matriz de covariância no Filtro de Kalman \textit{Unscented}}
	\symbl{$\mathbf{P}^{\cdot\cdot}_{n|n}$}{Estimação \textit{a posteriori} da matriz de covariância no Filtro de Kalman \textit{Unscented}}
\end{equation}
Curiosamente, a matriz de covariância $\mathbf{P}^{\mathbf{x}\mathbf{x}}_{n+1|n}$ não é utilizada na versão generalizada do filtro descrita, mas, de todo modo, ela é uma importante estatística do método e deve ser considerada.

Com os valores previstos, é possível também fazer uma predição da saída,
$\hat{\mathbf{z}}_{n+1|n}$. Porém, ao contrário dos estados, o valor da saída em $n+1$
é acessível, e contém informações do real comportamento do sistema. Assim, em posse da
saída, podemos realizar a etapa de \emph{correção}. Nela, as amostras estimadas são
corrigidas considerando o valor de $\mathbf{z}_{n+1}$, gerando assim
$\hat{\mathbf{x}}_{n+1|n+1}$ e $\mathbf{P}^{\mathbf{x}\mathbf{x}}_{n+1|n+1}$ (além de
$\hat{\mathbf{z}}_{n+1|n+1}$). As equações dessa etapa são caracterizadas pelo vetor de
erro entre a saída prevista e a real,
\begin{equation}
	\label{eq:unscented:diff}
	\bm{\nu}_{n+1} = \mathbf{z}_{n+1} - \hat{\mathbf{z}}_{n+1|n},
	\symbl{$\bm{\nu}_n$}{Vetor de inovação no Filtro de Kalman \textit{Unscented}}
\end{equation}
e uma matriz chamada de \emph{ganho de Kalman}, definida como sendo
\begin{equation}
	\label{eq:unscented:gain}
	\mathbf{W}_{n+1} = \mathbf{P}^{\mathbf{x}\mathbf{z}}_{n+1|n} (\mathbf{P}^{\bm{\nu}\bm{\nu}}_{n+1|n})^{-1}.
	\symbl{$\mathbf{W}_{n}$}{Ganho de Kalman}
\end{equation}
Com esses dois componentes, as correções nas estimativas são realizadas por meio das equações lineares abaixo~\cite{julier-1997}:
\begin{align}
	\label{eq:unscented:kalman-correction}
	\hat{\mathbf{x}}_{n+1|n+1}                  & = \hat{\mathbf{x}}_{n+1|n} + \mathbf{W}_{n+1} \bm{\nu}_{n+1},                                                            \\
	\mathbf{P}^{\mathbf{x}\mathbf{x}}_{n+1|n+1} & = \mathbf{P}^{\mathbf{x}\mathbf{x}}_{n+1|n} - \mathbf{W}_{n+1} \mathbf{P}^{\bm{\nu}\bm{\nu}}_{n+1|n} \mathbf{W}_{n+1}^T.
	\label{eq:unscented:kalman-last}
\end{align}

O leitor familiarizado com o filtro talvez estranhe as
Equações~\eqref{eq:unscented:kalman-first} a \eqref{eq:unscented:kalman-last}, visto
que são um tanto diferentes das comumente utilizadas. Como estamos partindo de um
modelo não-linear, as fórmulas precisaram ser generalizadas para além da representação
em um espaço de estados linear; de todo modo, a lógica ainda é a mesma: as etapas de
predição e correção são executadas em alternância, fazendo com que o comportamento
estatístico dos estados seja calculado recursivamente. As
Equações~\eqref{eq:unscented:kalman-correction} e~\eqref{eq:unscented:kalman-last}
merecem atenção redobrada: observe como o ganho de Kalman controla a relevância do erro
de observação na correção; essa característica é particularmente interessante para
sistemas com observações muito ruidosas, pois, nesses casos, o ganho aplicado ao vetor
de inovação será menor, e consequentemente a correção vai priorizar as predições em
detrimento de observações dúbias.

Por fim, note que as Equações~\eqref{eq:unscented:kalman-first} a
\eqref{eq:unscented:kalman-last} são dependentes das estimativas dos dois primeiros
momentos de $\mathbf{x}_n$ e $\mathbf{z}_n$. Assim, podemos concluir que \emph{parte do
	problema de aplicar o Filtro de Kalman a um sistema não-linear recai no problema mais
	genérico de estimar a média e a covariância de uma variável aleatória resultante de uma
	transformação não-linear.}

\section{A transformação \textit{unscented}}

Para estimar os momentos de $\mathbf{x}_{n}$ e $\mathbf{z}_n$, poderíamos linearizar o
sistema em torno de seu ponto de operação e então usufruir da linearidade dos
operadores estatísticos; essa é justamente a lógica por trás do Filtro de Kalman
Estendido, por exemplo. Porém, é possível demonstrar~\cite{julier-1997} que essa
abordagem considera apenas o primeiro componente das expansões em séries de potências
das funções não-lineares. Em outras palavras, é pressuposto que as demais ordens são
desprezíveis --- o que muitas vezes não é verdade, resultando em aproximações
equivocadas.

Diferentemente do método que acabamos de descrever, a transformação \textit{unscented}
se propõe a estimar os momentos da variável aleatória mapeada sem a necessidade de
aproximar a função não-linear. Para isso, o algoritmo gera um conjunto determinístico
de pontos a partir da média e da covariância da variável original, e então aplica o
mapeamento não-linear a esses pontos. As estatísticas são estimadas calculando-se
médias ponderadas dos valores mapeados.

A definição mais comumente associada à transformação encontra-se presente
em~\cite{julier-2002}; aqui, porém, discutiremos o algoritmo apresentado
em~\cite{julier-1997}, que é de mais fácil compreensão. Seja então $\mathbf{a} \in
	\mathbb{R}^k$ uma variável aleatória qualquer, com média $\Bar{\mathbf{a}}$ e matriz de
covariância $\mathbf{P}^{\mathbf{a}\mathbf{a}}$, e $\mathbf{g}$ uma função não-linear.
Nosso objetivo é estimar os dois primeiros momentos da variável aleatória
\begin{equation}
	\mathbf{b} = \mathbf{g}(\mathbf{a}).
\end{equation}

Para isso, primeiramente é gerado um conjunto de $2k + 1$ pontos, chamados de
\emph{pontos sigma}, e definidos como sendo
\begin{align}
	\label{eq:unscented:sigma-first}
	\symbl{$\lambda$}{Hiperparâmetro da transformação \textit{unscented}}
	\bm{\mathcal{A}}_0     & = \Bar{\mathbf{a}},                                                               \\
	\bm{\mathcal{A}}_{i}   & = \Bar{\mathbf{a}} + (\sqrt{(k + \lambda) \mathbf{P}^{\mathbf{a}\mathbf{a}}})_i,  \\
	\bm{\mathcal{A}}_{i+k} & = \Bar{\mathbf{a}} - ( \sqrt{(k + \lambda) \mathbf{P}^{\mathbf{a}\mathbf{a}}})_i,
	\label{eq:unscented:sigma-last}
\end{align}
onde $i \in \mathbb{N}$ varia de $1$ até $k$, $\lambda \in \mathbb{R}$ é um hiperparâmetro usado para ajustar a qualidade da estimação (para $\mathbf{a}$ gaussiana, um valor recomendado é $\lambda = 3 - k$~\cite{julier-1997}), e $(\sqrt{(k + \lambda) \mathbf{P}^{\mathbf{a}\mathbf{a}}})_i$ é a $i$-ésima coluna da raiz quadrada da matriz $(k + \lambda) \mathbf{P}^{\mathbf{a}\mathbf{a}}$. Cada ponto tem também um ganho associado:
\begin{equation}
	\symbl{$w_i$}{Coeficiente associado ao $i$-ésimo ponto sigma da transformação \textit{unscented}}
	w_i = \begin{cases}
		\lambda / (k + \lambda),  & i = 0;    \\
		\lambda / 2(k + \lambda), & i \neq 0.
	\end{cases}
	\label{eq:unscented:weights}
\end{equation}

Em cada ponto sigma, aplica-se a função $\mathbf{g}$, assim gerando um novo conjunto de
pontos transformados descritos por
\begin{equation}
	\bm{\mathcal{B}}_i = \mathbf{g}(\bm{\mathcal{A}}_i).
\end{equation}

Então, o valor médio e a matriz de covariância de $\mathbf{b}$ são estimados por meio
de médias ponderadas com os pontos transformados:
\begin{gather}
	\Bar{\mathbf{b}} = \sum_{i=0}^{2k} w_i \bm{\mathcal{B}}_i, \\
	\mathbf{P}^{\mathbf{b}\mathbf{b}} = \sum_{i=0}^{2k} w_i (\bm{\mathcal{B}}_i - \Bar{\mathbf{b}}) (\bm{\mathcal{B}}_i - \Bar{\mathbf{b}})^T.
\end{gather}

Dentre as propriedades da transformação \textit{unscented}, a principal é a
incorporação de mais ordens de $\Bar{\mathbf{a}}$ e $\mathbf{P}^{\mathbf{a}\mathbf{a}}$
na estimação das estatísticas, o que resulta em uma aproximação melhor que --- ou, no
pior caso, tão boa quanto --- a estimada a partir de um processo de
linearização~\cite{julier-1997}. Outra característica muito atrativa do método é sua
inacreditável simplicidade: o algoritmo é imediatamente aplicável a \emph{qualquer}
tipo de mapeamento não-linear, ao passo que na linearização precisamos derivar o
jacobiano associado à função $\mathbf{g}$. A Figura~\ref{fig:unscented:ut} ilustra os
dois métodos de estimação discutidos na seção, junto à estratégia trivial (e, na
maioria das vezes, irrealizável) de gerar um conjunto considerável de amostras de
$\mathbf{b}$ para computar os momentos.

\begin{figure}[!ht]
	\centering
	\input{images/unscented/estimation-methods}
	\caption[Ilustração dos métodos de estimação de momentos]{Três modos de estimar os momentos da variável aleatória $\mathbf{b}$. À esquerda, (muitas) amostras do experimento são usadas para calcular as estatísticas reais da variável. Ao centro, os momentos são aproximados pelo modelo linearizado da função ($\mathbf{J}$ é o jacobiano de $\mathbf{g}$); erros na aproximação acabam sendo propagados para a estimação. À direita, temos a transformação \textit{unscented}, na qual um conjunto estratégico de pontos é utilizado para computar as estatísticas. Ilustração baseada em um desenho similar presente em~\cite{wan-2000}.}
	\label{fig:unscented:ut}
\end{figure}

\section{Filtro de Kalman \textit{Unscented}}

Com o problema bem definido e a transformação \textit{unscented} apresentada, derivar o
filtro se torna relativamente simples. Tudo que é preciso fazer é encontrar as equações
associadas às previsões $\hat{\mathbf{x}}_{n+1|n}$,
$\mathbf{P}_{n+1|n}^{\mathbf{x}\mathbf{x}}$, $\hat{\mathbf{z}}_{n+1|n}$,
$\mathbf{P}_{n+1|n}^{\bm{\nu}\bm{\nu}}$, e $\mathbf{P}_{n+1|n}^{\mathbf{x}\mathbf{z}}$,
para que seja possível realizar as etapas de predição e correção alternadamente.

Precisamos primeiramente definir um novo vetor de estados, no qual se inclua
$\mathbf{v}_n$. Deste modo, as estatísticas do ruído ``interno'' --- modelado como
não-aditivo --- serão consideradas na estimação da média e da covariância do sistema.
Assim, supondo que $\mathbf{v}_n \in \mathbb{R}^q$, o vetor de estados aumentado
$\mathbf{x}^a_{n} \in \mathbb{R}^{(k + q)}$ é definido como
\begin{equation}
	\mathbf{x}^a_{n} = \begin{bmatrix}
		\mathbf{x}_n \\
		\mathbf{v}_n
	\end{bmatrix}.
	\symbl{$\mathbf{x}^a_{n}$}{Vetor de estados aumentado modelado pelo Filtro de Kalman \textit{Unscented}}
\end{equation}
Consequentemente, temos que a dinâmica do sistema agora é descrita por
\begin{equation}
	\mathbf{x}^a_{n+1} = \mathbf{f}(\mathbf{x}^a_{n}, \mathbf{u}_n),
\end{equation}
e a média e a covariância a serem usadas pela transformação \textit{unscented} serão
\begin{equation}
	\hat{\mathbf{x}}^a_{n|n} = \begin{bmatrix}
		\mathbf{x}_{n|n} \\
		\mathbf{0}
	\end{bmatrix},\
	\mathbf{P}^{\mathbf{x}^a\mathbf{x}^a}_{n|n} = \begin{bmatrix}
		\mathbf{P}^{\mathbf{x}\mathbf{x}}_{n|n} & \mathbf{P}^{\mathbf{x}\mathbf{v}}_{n|n} \\
		\mathbf{P}^{\mathbf{x}\mathbf{v}}_{n|n} & \mathbf{Q}_n
	\end{bmatrix}.
	\label{eq:unscented:augmented}
\end{equation}

Munidos de todas as definições necessárias, podemos agora descrever o algoritmo do
Filtro de Kalman \textit{Unscented}. O passo de inicialização consiste em calcular as
condições iniciais dos estados originais com
\begin{align}
	\hat{\mathbf{x}}_{0|0}                  & = E\{ \mathbf{x}_0 \},                                                                      \\
	\mathbf{P}_{0|0}^{\mathbf{x}\mathbf{x}} & = E\{ (\mathbf{x}_0 - \hat{\mathbf{x}}_{0|0}) (\mathbf{x}_0 - \hat{\mathbf{x}}_{0|0})^T \},
\end{align}
para que o sistema aumentado comece com as condições $\hat{\mathbf{x}}^a_{0|0}$ e $\mathbf{P}^{\mathbf{x}^a\mathbf{x}^a}_{0|0}$ como descritas pela Equação~\eqref{eq:unscented:augmented}. Então, para $n \in \mathbb{N}$ variando de $0$ a infinito (ou até a última amostra de interesse), iteramos pelos seguintes passos:

\begin{enumerate}
	\item Em posse de $\hat{\mathbf{x}}^a_{n|n}$, estima-se a observação
	      $\hat{\mathbf{z}}_{n|n}$, que é a saída do filtro.

	\item Com as Equações~\eqref{eq:unscented:sigma-first} a~\eqref{eq:unscented:sigma-last}, são
	      calculados os $2(k + q) + 1$ pontos sigma associados a $\hat{\mathbf{x}}^a_{n|n}$,
	      gerando assim o conjunto $\{\bm{\mathcal{X}}_{n|n,i}^a\}$.

	\item Os pontos sigma são mapeados pela função não-linear que descreve a dinâmica dos estados
	      do sistema, ou seja, \symbl{$\bm{\mathcal{X}}_{\cdot,i}$}{Ponto sigma $i$ da entrada}
	      \begin{equation}
		      \bm{\mathcal{X}}_{n+1|n,i}^a = \mathbf{f}(\bm{\mathcal{X}}_{n|n,i}^a, \mathbf{u}_n).
	      \end{equation}

	\item Com os coeficientes $w_i$ definidos na Equação~\eqref{eq:unscented:weights}, o novo
	      vetor de estados e sua matriz de covariância são estimados como sendo
	      \begin{gather}
		      \hat{\mathbf{x}}_{n+1|n}^a = \sum_{i=0}^{2(k+q)} w_i \bm{\mathcal{X}}_{n+1|n,i}^a, \\
		      \mathbf{P}^{\mathbf{x}^a\mathbf{x}^a}_{n+1|n} = \sum_{i=0}^{2(k+q)} w_i (\bm{\mathcal{X}}_{n+1|n,i}^a - \hat{\mathbf{x}}_{n+1|n}^a) (\bm{\mathcal{X}}_{n+1|n,i}^a - \hat{\mathbf{x}}_{n+1|n}^a)^T.
	      \end{gather}

	\item Os pontos sigma são novamente mapeados, agora pela função que descreve a dinâmica de
	      saída do sistema:
	      \begin{equation}
		      \symbl{$\bm{\mathcal{Z}}_{\cdot,i}$}{Ponto sigma $i$ da observação}
		      \bm{\mathcal{Z}}_{n+1|n,i} = \mathbf{h}(\bm{\mathcal{X}}_{n+1|n,i}^a, \mathbf{u}_{n+1}).
	      \end{equation}

	\item Com os pontos mapeados, estimam-se a próxima observação e sua covariância (lembrando-se
	      de que o ruído $\mathbf{w}_n$ é pressuposto aditivo e independente),
	      \begin{gather}
		      \hat{\mathbf{z}}_{n+1|n} = \sum_{i=0}^{2(k+q)} w_i \bm{\mathcal{Z}}_{n+1|n,i}, \\
		      \mathbf{P}^{\bm{\nu}\bm{\nu}}_{n+1|n} = \mathbf{R}_n + \sum_{i=0}^{2(k+q)} w_i (\bm{\mathcal{Z}}_{n+1|n,i} - \hat{\mathbf{z}}_{n+1|n}) (\bm{\mathcal{Z}}_{n+1|n,i} - \hat{\mathbf{z}}_{n+1|n})^T.
	      \end{gather}

	\item Por fim, calculamos a matriz de covariância cruzada entre $\mathbf{x}_n^a$ e
	      $\mathbf{z}$,
	      \begin{equation}
		      \mathbf{P}^{\mathbf{x}^a\mathbf{z}}_{n+1|n} = \sum_{i=0}^{2(k+q)} w_i (\bm{\mathcal{X}}_{n+1|n,i}^a - \hat{\mathbf{x}}_{n+1|n}^a) (\bm{\mathcal{Z}}_{n+1|n,i} - \hat{\mathbf{z}}_{n+1|n})^T.
	      \end{equation}

	\item A observação $\mathbf{z}_{n+1}$ é usada, em conjunto com as
	      Equações~\eqref{eq:unscented:diff} a \eqref{eq:unscented:kalman-last} e as matrizes
	      calculadas nas etapas anteriores, na correção das estimações do vetor de estados e sua
	      matriz de covariância, gerando assim $\hat{\mathbf{x}}_{n+1|n+1}^a$ e
	      $\mathbf{P}^{\mathbf{x}^a\mathbf{x}^a}_{n+1|n+1}$.
\end{enumerate}

Esse algoritmo é específico para o modelo descrito na
Seção~\ref{section:unscented:formulation}, mas pode ser facilmente adaptado a outras
hipóteses. Para considerar um ruído de observação não-aditivo, por exemplo, poderíamos
definir uma versão aumentada do vetor de observação, a qual incluiria o vetor de ruído.

\section{Método proposto}
\label{section:unscented:model}

Durante toda a análise do filtro, pressupusemos que as funções não-lineares
$\mathbf{f}$ e $\mathbf{h}$ eram previamente conhecidas. Afinal, o algoritmo é
utilizado para estimar a memória do sistema, não sua dinâmica. Porém, em um problema de
identificação, como o do trabalho, essa é justamente a incógnita que se quer descobrir.
Assim, precisamos adaptar a estratégia para esse ângulo.

Quando nada se sabe sobre o comportamento interno do sistema, o que podemos fazer é
definir funções --- idealmente genéricas, capazes de aproximar outros mapeamentos ---
que dependam dos estados e de um conjunto de parâmetros. Então, dois filtros são
executados de forma simultânea: um para a estimação dos estados, e outro para a
estimação dos parâmetros. Este método é chamado de \emph{Filtro de Kalman Unscented
	Duplo}~\cite{wan-2000}. O primeiro subsistema é muito similar ao apresentado na
Seção~\ref{section:unscented:formulation}, porém com uma entrada adicional, que é vetor
de parâmetros $\bm{\omega}_n$\symbl{$\bm{\omega}_n$}{Vetor com os coeficientes do
	polinômio de Volterra usado no Filtro de Kalman \textit{Unscented}}:
\begin{align}
	\mathbf{x}_{n+1} & = \mathbf{f}(\mathbf{x}_n, \mathbf{u}_n, \bm{\omega}_n, \mathbf{v}_n),  \\
	\mathbf{z}_n     & = \mathbf{h}(\mathbf{x}_n, \bm{\omega}_n, \mathbf{u}_n) + \mathbf{w}_n.
\end{align}
Já o subsistema associado aos parâmetros é descrito por
\begin{align}
	\bm{\omega}_{n+1} & = \bm{\omega}_n + \mathbf{v}_n,                                         \\
	\mathbf{z}_n      & = \mathbf{h}(\mathbf{x}_n, \bm{\omega}_n, \mathbf{u}_n) + \mathbf{w}_n.
\end{align}
Embora pareça que a dinâmica desse subsistema acarrete na convergência dos coeficientes, o ruído de processo e o algoritmo de correção contribuem com uma ``inovação'' a cada nova amostra, resultando assim num filtro adaptativo.

Vamos agora finalmente associar a teoria apresentada aos elementos específicos do
projeto. Para a função $h$ (escalar, já que a saída é unidimensional), optamos por usar
um polinômio de Volterra~\cite{ogunfunmi-2007} de $P$-ésima ordem\symbl{$P$}{Ordem do
	polinômio de Volterra} que utilize as últimas $L$ amostras da entrada $x[n]$, ou seja,
\begin{equation}
	\symbl{$h(\dots)$}{Polinômio de Volterra do Filtro de Kalman \textit{Unscented}}
	h(\mathbf{x}_n, \bm{\omega}_n, \mathbf{u}_n) = \sum_{m=0}^{L-1} \omega_m x[n-m] + \sum_{m=0}^{L-1} \sum_{p=m}^{L-1} \omega_{m,p} x[n-m] x[n-p] + \dots.
\end{equation}
Dois fatores justificam essa escolha: em primeiro lugar, polinômios são capazes de aproximar (localmente) diversos mapeamentos complexos, portanto são funções genéricas o suficiente para serem usadas na modelagem de um sistema desconhecido. Além disso, os termos cruzados fazem com que a ``correlação instantânea'' entre amostras seja considerada na estimação, o que pode contribuir na reprodução de processamentos não-lineares com memória. Porém, ressaltamos o que foi dito no Capítulo~\ref{chapter:intro}: os coeficientes desse modelo crescem exponencialmente com a ordem do polinômio, portanto, devemos tomar cuidado na escolha de $L$ e $P$.

Com a dinâmica da saída definida, precisamos então caracterizar cada um dos vetores do
sistema. A entrada será simplesmente o sinal não-distorcido,
\begin{equation}
	\mathbf{u}_n = x[n].
\end{equation}

O vetor $\bm{\omega}_n$ será composto por todos os parâmetros do polinômio,
\begin{equation}
	\symbl{$\omega_{i,\dots,j}$}{Coeficiente do polinômio de Volterra usado no Filtro de Kalman \textit{Unscented}}
	\bm{\omega}_n = \begin{bmatrix}
		\omega_0 & \cdots & \omega_{L-1} & \omega_{0,0} & \cdots & \omega_{L-1,\dots,L-1}
	\end{bmatrix}^T.
\end{equation}

Para que as amostras passadas possam ser utilizadas pela função, elas precisam ser
armazenadas na memória do sistema. Assim, o vetor de estados será composto pelas $L-1$
amostras anteriores da entrada,
\begin{equation}
	\mathbf{x}_n = \begin{bmatrix}
		x[n-1] & x[n-2] & \cdots & x[n-L+1]
	\end{bmatrix}^T.
\end{equation}
Desse modo, a dinâmica interna do primeiro subsistema, descrita pela função $\mathbf{f}$, consiste apenas na inclusão de $x[n]$ como novo primeiro elemento do vetor de estados, e no deslocamento das demais amostras --- resultando no descarte de $x[n-L]$. O ruído de processo $\mathbf{v}_n$ é considerado aditivo em ambos os subsistemas.

Por fim, pode-se afirmar então que a o sinal desejado é descrito por
\begin{equation}
	d[n] = h(\mathbf{x}_n, \bm{\omega}_n, \mathbf{u}_n),
\end{equation}
porém temos apenas a observação ruidosa desse sinal, $\mathbf{z}_n = y[n] = d[n] + w_n$.

\section{Experimentos}

Em nossa última leva de experimentos, foram novamente utilizadas as duas faixas de
áudio apresentadas na Seção~\ref{section:wf:experiments}: como sinal original, uma
breve gravação da flauta hulusi (espectrograma na
Figura~\ref{fig:wf:spectogram-reference}); como ruído, um diálogo de uma leitura
dramática de \textit{Mansfield Park} (espectrograma na
Figura~\ref{fig:wf:spectogram-dialogue}). Ambas as gravações permaneceram com os
pré-condicionamentos anteriormente especificados (mono, taxa de amostragem de
$44.1$~kHz, \textit{bitdepth} de 16~bits, ganho de normalização para 0~dB).

Desde a versão R2016b, o MATLAB conta com uma função nativa que implementa o Filtro de
Kalman \textit{Unscented}, \texttt{unscentedKalmanFilter()}. Assim, o algoritmo foi
elaborado instanciando dois filtros, um associado ao subsistema dos estados e outro
associado ao subsistema dos coeficientes. Quase todos os valores padrão da função foram
mantidos; apenas a variância do ruído de processo ($\sigma_{\mathbf{v}}^2 =
	0.01$\symbl{$\sigma_{\mathbf{v}}^2$}{Variância do ruído de processo} para todos os
experimentos) e a variância do ruído de saída (definidas individualmente) foram
modificadas. Os filtros foram inicializados com vetores nulos. Por fim, foram
estipulados $P=3$ e $L=3$ para o polinômio de Volterra; assim como no Filtro de
Correntropia, valores maiores aumentavam de modo considerável o custo computacional do
método, e resultavam em estimações apenas marginalmente melhores.

\subsection{\textit{Fades}}

Para testar a capacidade do algoritmo em identificar um sistema variante no tempo,
foram aplicados à gravação original um \textit{fade-out} (com parâmetros $A_\text{i} =
1$ e $A_\text{f} = 0.2$) e um \textit{fade-in} (com parâmetros $A_\text{i} = 0.2$ e
$A_\text{f} = 1$) em seu início e seu fim, respectivamente; cada efeito seguiu uma
curva exponencial e durou cerca de 1.5 segundo. Nenhum ruído foi introduzido, porém a
variância do ruído de saída foi considerada ser $\sigma_w^2 =
	10^{-4}$\symbl{$\sigma_{\mathbf{w}}^2$}{Variância do ruído de saída}. Os resultados
encontram-se na Tabela~\ref{tab:unscented:experiment-1}.
{\def\arraystretch{1.25}\tabcolsep=10pt
\begin{table}[!ht]
	\centering
	\caption[Resultados do primeiro experimento: \textit{fades}]{Resultados do primeiro experimento.}
	\label{tab:unscented:experiment-1}
	\begin{tabular}{cccc}
		\toprule
		               & SDR         & PAQM (MOS) & $\log_{10}(R_{\text{nonlin}})$ \\
		\midrule
		$(d, x)$       & $-11.57$ dB & $1.17$     & $-0.005$                       \\
		$(d, \hat{d})$ & $98.25$ dB  & $4.62$     & $-0.004$                       \\ \bottomrule
	\end{tabular}
\end{table}
}

O filtro não só foi capaz de acompanhar o processamento, como superou os resultados de
ambos os métodos discutidos anteriormente. A provável justificativa para a qualidade da
estimativa é o fato de que os coeficientes do polinômio são corrigidos de amostra em
amostra --- não de janela em janela, como nas estratégias propostas para os outros dois
filtros ---, dando assim mais dinamismo à estimação.

\subsection{\textit{Fades} com ruído aditivo}

Neste experimento, o diálogo foi somado à gravação gerada na subseção anterior, fazendo
assim com que o sinal desejado se comportasse como trilha de fundo da mixagem. A SNR
resultante foi de $-9.17$~dB. A variância do ruído de saída ótima para esse experimento
(estipulada após diversos testes com diferentes valores) foi de $\sigma_w^2 = 4 \times
	10^{4}$. Os resultados encontram-se na Tabela~\ref{tab:unscented:experiment-2}.
{\def\arraystretch{1.25}\tabcolsep=10pt
\begin{table}[!ht]
	\centering
	\caption[Resultados do segundo experimento: \textit{fades} com ruído aditivo]{Resultados do segundo experimento.}
	\label{tab:unscented:experiment-2}
	\begin{tabular}{cccc}
		\toprule
		               & SDR         & PAQM (MOS) & $\log_{10}(R_{\text{nonlin}})$ \\
		\midrule
		$(d, x)$       & $-11.57$ dB & $1.17$     & $-0.005$                       \\
		$(d, \hat{d})$ & $16.56$ dB  & $1.18$     & $-0.027$                       \\ \bottomrule
	\end{tabular}
\end{table}
}

No Capítulo~\ref{chapter:wiener}, vimos que a estimação gerada pelo Filtro de Wiener
apresentou artefatos ``metálicos'' e ininteligíveis. Em contrapartida, ao se ouvir o
sinal computado pelo Filtro de Kalman \textit{Unscented}, nota-se que o ruído ficou
abafado, com baixa potência, mas ainda assim compreensível (ou seja, foi possível
identificar as falas dos personagens).

\subsection{\textit{Soft clipping}}

Para avaliarmos a capacidade do filtro em estimar um processamento não-linear,
aplicou-se o efeito de \textit{soft clipping} à gravação da flauta. O pico de amplitude
foi limitado a $-7.5$~dB. A variância do ruído de saída foi novamente $\sigma_w^2 =
	10^{-4}$. Os resultados encontram-se na Tabela~\ref{tab:unscented:experiment-3}.
{\def\arraystretch{1.25}\tabcolsep=10pt
\begin{table}[!ht]
	\centering
	\caption[Resultados do terceiro experimento: \textit{soft clipping}]{Resultados do terceiro experimento.}
	\label{tab:unscented:experiment-3}
	\begin{tabular}{cccc}
		\toprule
		               & SDR        & PAQM (MOS) & $\log_{10}(R_{\text{nonlin}})$ \\
		\midrule
		$(d, x)$       & $19.56$ dB & $1.21$     & $-0.016$                       \\
		$(d, \hat{d})$ & $99.04$ dB & $4.62$     & $-0.003$                       \\ \bottomrule
	\end{tabular}
\end{table}
}

Não há muito o que comentar em relação aos resultados, além de que o filtro reproduziu
satisfatoriamente o processamento aplicado. Não foi possível identificar nenhum
artefato presente na estimação como ocorreu com os outros dois métodos.

\subsection{\textit{Soft clipping} com ruído aditivo}

Neste experimento, queremos novamente que o filtro estime o efeito de \textit{soft
	clipping}, porém agora com ruído aditivo adicionado à mixagem. A SDR da observação foi
de $3.97$~dB. Para esse teste, a variância de ruído ótima (dentre os valores testados)
foi de $\sigma_w^2 = 10^{4}$. Os resultados encontram-se na
Tabela~\ref{tab:unscented:experiment-4}. {\def\arraystretch{1.25}\tabcolsep=10pt
\begin{table}[!ht]
	\centering
	\caption[Resultados do quarto experimento: \textit{soft clipping} com ruído aditivo]{Resultados do quarto experimento.}
	\label{tab:unscented:experiment-4}
	\begin{tabular}{cccc}
		\toprule
		               & SDR        & PAQM (MOS) & $\log_{10}(R_{\text{nonlin}})$ \\
		\midrule
		$(d, x)$       & $19.56$ dB & $1.21$     & $-0.016$                       \\
		$(d, \hat{d})$ & $27.27$ dB & $1.18$     & $-0.016$                       \\ \bottomrule
	\end{tabular}
\end{table}
}

Houve uma melhoria no valor da SDR, e, curiosamente, uma queda na PAQM. Ao se ouvir a
estimação resultante, observa-se um comportamento similar ao do segundo experimento: o
filtro conseguiu reproduzir o efeito aplicado ao sinal original, mas não removeu por
completo a faixa de diálogo, que ficou agressivamente abafada.

\subsection{\textit{Fades} e codificação com perdas}

Nosso objetivo agora é avaliar a capacidade do método de reproduzir não só um
processamento linear, como também um não-linear aplicado em seguida. Para isso, a
gravação gerada no primeiro experimento foi comprimida com uma codificação MP3 com taxa
de bits constante de 56~kbps. Assim como nos outros testes em que o diálogo não foi
adicionado, a variância do ruído de saída foi estipulada como sendo $\sigma_w^2 =
	10^{-4}$. Os resultados encontram-se na Tabela~\ref{tab:unscented:experiment-5}.
{\def\arraystretch{1.25}\tabcolsep=10pt
\begin{table}[!ht]
	\centering
	\caption[Resultados do quinto experimento: \textit{fades} e codificação com perdas]{Resultados do quinto experimento.}
	\label{tab:unscented:experiment-5}
	\begin{tabular}{cccc}
		\toprule
		               & SDR         & PAQM (MOS) & $\log_{10}(R_{\text{nonlin}})$ \\
		\midrule
		$(d, x)$       & $-14.77$ dB & $1.17$     & $-0.032$                       \\
		$(d, \hat{d})$ & $66.78$ dB  & $4.50$     & $-0.004$                       \\ \bottomrule
	\end{tabular}
\end{table}
}

De todos os experimentos executados neste projeto, este talvez seja o mais
surpreendente. O processamento aplicado por um algoritmo de compressão com perdas é
altamente complexo; assim, é no mínimo admirável o nível de fidelidade da estimação do
filtro, embora parte desses resultados seja consequência da emulação da distorção
linear.

\subsection{\textit{Fades} com ruído aditivo e codificação com perdas}

Neste último experimento, queremos verificar se o filtro ainda é capaz de reproduzir o
efeito da codificação com perdas quando ruído aditivo é introduzido antes do
processamento. Para gerar o sinal observado, aplicou-se a codificação MP3 de 56~kbps na
mixagem do segundo experimento. Após testarmos diferentes valores, a variância do ruído
da saída foi estipulada como sendo $\sigma_w^2 = 2 \times 10^{3}$. Não foi possível
calcular a SNR dessa observação, mas podemos estimar seu limite superior como sendo
$-9.17$~dB. Os resultados encontram-se na Tabela~\ref{tab:unscented:experiment-6}.
{\def\arraystretch{1.25}\tabcolsep=10pt
\begin{table}[!ht]
	\centering
	\caption[Resultados do sexto experimento: \textit{fades} com ruído aditivo e codificação com perdas]{Resultados do sexto experimento.}
	\label{tab:unscented:experiment-6}
	\begin{tabular}{cccc}
		\toprule
		               & SDR         & PAQM (MOS) & $\log_{10}(R_{\text{nonlin}})$ \\
		\midrule
		$(d, x)$       & $-14.77$ dB & $1.17$     & $-0.032$                       \\
		$(d, \hat{d})$ & $5.85$ dB   & $1.17$     & $-0.148$                       \\ \bottomrule
	\end{tabular}
\end{table}
}

Como podemos ver, a única medida que apresentou melhoria foi a SDR. Ao ouvirmos a
gravação resultante, percebe-se que foi introduzido um ruído ``metálico'' constante
pelo método; além disso, o diálogo se manteve presente (embora abafado, como nos outros
casos), e foi possível ouvir a presença de distorções indesejadas ao sinal de interesse
(muito similares ao efeito de \textit{clipping}).
